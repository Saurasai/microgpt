## Transformer Language Model

This repository contains code for training and generating text using a language model based on a bigram approach with a transformer architecture.

### Overview

This script performs the following steps:

1. Data Preprocessing: Reads input text data from a file, preprocesses it, and splits it into training and validation sets.
2. Model Definition: Defines PyTorch modules for building the transformer-based language model, including self-attention layers, feedforward layers, and transformer blocks.
3. Model Training: Trains the language model using the specified hyperparameters and optimizer.
4. Text Generation: Generates text using the trained language model.

### Requirements

- PyTorch (>=1.9.0): [https://pytorch.org/](https://pytorch.org/)

### Usage

1. Clone the repository:

    ```bash
    git clone https://github.com/yourusername/bigram-transformer-language-model.git
    ```

2. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

3. Run the script:

    ```bash
    python train_and_generate.py
    ```

### Hyperparameters

- Batch size
- Block size
- Maximum iterations
- Evaluation interval
- Learning rate
- Number of attention heads
- Number of transformer layers
- Dropout probability

### Dataset

The script expects the input text data to be stored in a file named `input.txt`.

### License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
